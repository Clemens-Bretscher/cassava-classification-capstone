{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, cv2,glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,fbeta_score,confusion_matrix,plot_confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cbb</th>\n",
       "      <th>cbsd</th>\n",
       "      <th>cgm</th>\n",
       "      <th>cmd</th>\n",
       "      <th>healthy</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>466</td>\n",
       "      <td>1443</td>\n",
       "      <td>773</td>\n",
       "      <td>2658</td>\n",
       "      <td>316</td>\n",
       "      <td>5656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cbb  cbsd  cgm   cmd  healthy  total\n",
       "0  466  1443  773  2658      316   5656"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/data_info.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our dataset the probability of getting CMD(3) is **46.99%**,that of CBB(0) **8.24%**, CBSD(1) **25.51%**, CGM(2) **13.67%** and a Healthy(4) one **5.59%**. Our baseline model is a probability function where its prediction is based on a label with the highest probability rate which is CMD. However, due to the imbalanced nature of our dataset accuracy can be a misleading metric in our modeling.\n",
    "\n",
    "In the following estimation of the F_beta-score we assigned the \"average\" parameter to \"macro\" since our dataset is imbalanced. This will put more emphasis on the false negative or class II error. See below for the explanation of the values of the parameter:\n",
    "\n",
    "**'binary':**\n",
    "Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred}) are binary.\n",
    "\n",
    "**'micro':**\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "**'macro':**\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "**'weighted':**\n",
    "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "**'samples':**\n",
    "Calculate metrics for each instance, and find their average (only meaningful for multi-label classification where this differs from accuracy_score).\n",
    "\n",
    "In the case of weighted average the performance metrics are weighted accordingly:\n",
    "\n",
    "$score_{weighted\\text{-}avg} = 0.0824 \\cdot score_{class\\text{ }0} + 0.0.2551 \\cdot score_{class\\text{ }1} + 0.1367 \\cdot score_{class\\text{ }2} + ...$\n",
    "\n",
    "Which will give us a higher F2 score due the class imbalance.\n",
    "\n",
    "However, macro avg is not weighted and therefore:\n",
    "\n",
    "$score_{macro\\text{-}avg} = 0.5 \\cdot score_{class\\text{ }0} + 0.5 \\cdot score_{class\\text{ }1} + 0.5 \\cdot score_{class\\text{ }2} + ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of image labels and their relative path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dict = {\n",
    "                0: [i for i in glob.glob(os.path.join(path,'cbb/*.jpg'))],\n",
    "                1: [i for i in glob.glob(os.path.join(path,'cbsd/*.jpg'))],\n",
    "                2: [i for i in glob.glob(os.path.join(path,'cgm/*.jpg'))],\n",
    "                3: [i for i in glob.glob(os.path.join(path,'cmd/*.jpg'))],\n",
    "                4: [i for i in glob.glob(os.path.join(path,'healthy/*.jpg'))]\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data: create vector from raw image file and resize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3D tensor from images\n",
    "X, y = [], []\n",
    "\n",
    "for labels, images in images_dict.items():\n",
    "    for image in images:\n",
    "        img = cv2.imread(''.join(image))\n",
    "        resized_img = cv2.resize(img, dsize=(224,224))\n",
    "        X.append(resized_img)\n",
    "        y.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an numpy_array\n",
    "X=np.array(X)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale X\n",
    "X = X.astype(np.float32)/ 255.\n",
    "y = y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data segregation into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning, a simple sanity check consists of comparing one's estimator against simple rules of thumb. The target will be to beat the classifier that makes predictions using simple rules. DummyClassifier implements several such simple strategies for classification:\n",
    "\n",
    "- **stratified** generates random predictions by respecting the training set class distribution.\n",
    "\n",
    "- **most_frequent** always predicts the most frequent label in the training set.\n",
    "\n",
    "- **prior** always predicts the class that maximizes the class prior (like most_frequent) and predict_proba returns the class prior.\n",
    "\n",
    "- **uniform** generates predictions uniformly at random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(random_state=42, strategy='most_frequent')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy='most_frequent',random_state=42)\n",
    "dummy_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline accuracy : 0.46\n",
      "F_beta_score_recall: 0.16\n"
     ]
    }
   ],
   "source": [
    "y_pred = dummy_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "beta_score_recall = fbeta_score(y_test, y_pred, average='macro', beta=2)\n",
    "print(f'baseline accuracy : {accuracy.round(2)}')\n",
    "print(f'F_beta_score_recall: {beta_score_recall.round(2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(dummy_clf,X_test,y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, 108,   0],\n",
       "       [  0,   0,   0, 302,   0],\n",
       "       [  0,   0,   0, 147,   0],\n",
       "       [  0,   0,   0, 520,   0],\n",
       "       [  0,   0,   0,  55,   0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative method to predict the probability of the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline accuracy : 0.47\n",
      "F_beta_score_recall: 0.16\n"
     ]
    }
   ],
   "source": [
    "y_pred = [3] * len(data.label)\n",
    "score = accuracy_score(data.label, y_pred)\n",
    "beta_score_recall = fbeta_score(data.label , y_pred, average='macro', beta=2)\n",
    "print(f'baseline accuracy : {score.round(2)}')\n",
    "print(f'F_beta_score_recall: {beta_score_recall.round(2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "In a binary classification task, the terms \"positive\" and \"negative\" refer to the classifier's prediction, and the terms \"true\" and \"false\" refer to whether that prediction corresponds to the external judgment (sometimes known as the \"observation\").\n",
    "\n",
    "In this context, we can define the notions of precision, recall and F-measure:\n",
    "\n",
    "$\\text{precision} = \\frac{tp}{tp + fp}$,\n",
    "\n",
    "$\\text{recall} = \\frac{tp}{tp + fn}$,\n",
    "\n",
    "$F_\\beta = (1 + \\beta^2) \\frac{\\text{precision} \\times \\text{recall}}{\\beta^2 \\text{precision} + \\text{recall}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For beta ($\\beta$) = 2 and **stratified** strategy for the dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(random_state=42, strategy='stratified')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy='stratified',random_state=42)\n",
    "dummy_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline accuracy : 0.3\n",
      "F_beta_score_recall: 0.19\n"
     ]
    }
   ],
   "source": [
    "y_pred = dummy_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "beta_score_recall = fbeta_score(y_test, y_pred, average='macro', beta=2)\n",
    "print(f'baseline accuracy : {accuracy.round(2)}')\n",
    "print(f'F_beta_score_recall: {beta_score_recall.round(2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(dummy_clf,X_test,y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5,  25,  19,  51,   8],\n",
       "       [ 28,  65,  35, 153,  21],\n",
       "       [ 11,  37,  17,  74,   8],\n",
       "       [ 43, 123,  73, 248,  33],\n",
       "       [  4,  16,   9,  21,   5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5,  25,  19,  51],\n",
       "       [ 28,  65,  35, 153],\n",
       "       [ 11,  37,  17,  74],\n",
       "       [ 43, 123,  73, 248]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm[:4,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " True Negative for the healthy class = 1007\n"
     ]
    }
   ],
   "source": [
    "print(f' True Negative for the healthy class = {np.sum(cm[:4,:4])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 21,  8, 33])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm[:4,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " False positive for the healthy class = 70\n"
     ]
    }
   ],
   "source": [
    "print(f' False positive for the healthy class = {np.sum(cm[:4,4])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 16,  9, 21])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm[4,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " False negative for the healthy class = 50\n"
     ]
    }
   ],
   "source": [
    "print(f' False negative for the healthy class = {np.sum(cm[4,:4])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " True positive for the healthy class = 5\n"
     ]
    }
   ],
   "source": [
    "print(f' True positive for the healthy class = {np.sum(cm[4,4])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Precision: 0.067\n"
     ]
    }
   ],
   "source": [
    "precision = 5/(5+70)\n",
    "print(f'Classification Precision: {round(precision,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Sensitivity: 0.091\n"
     ]
    }
   ],
   "source": [
    "recall = 5/(5+50)\n",
    "print(f'Classification Sensitivity: {round(recall,3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2-score for the healthy class: 0.085\n"
     ]
    }
   ],
   "source": [
    "F2 = (1+pow(2,2))*(precision*recall/(pow(2,2)*precision+recall))\n",
    "print(f'F2-score for the healthy class: {round(F2,3)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18747c473b89d7884611ab29d392bcd79a1afb8f4635bd78d86e5ba1f512bf99"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
