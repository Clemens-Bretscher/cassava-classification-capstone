{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, cv2, json\n",
    "import opendatasets as od\n",
    "import zipfile\n",
    "\n",
    "# ignoring warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloads a source and unzip the zipped folder to a location on local drive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:Your Kaggle Key:Downloading cassava-leaf-disease-classification.zip to ./cassava-leaf-disease-classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.76G/5.76G [14:42<00:00, 7.01MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive ./cassava-leaf-disease-classification/cassava-leaf-disease-classification.zip to ./cassava-leaf-disease-classification\n"
     ]
    }
   ],
   "source": [
    "od.download(\"https://www.kaggle.com/c/cassava-leaf-disease-classification/data?select=train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes data is stored in a zip folder in side a zip folder. In this case we use the `od.download()` method to download a folder and unzip the zipped folder but the folder inside the zipped folder should be latter on unzipped with `zipfile.ZipFile(<origin>, 'r') and alias.extractall(<destination>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:Your Kaggle Key:Downloading cassava-disease.zip to ./cassava-disease\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.30G/2.30G [05:33<00:00, 7.42MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive ./cassava-disease/cassava-disease.zip to ./cassava-disease\n"
     ]
    }
   ],
   "source": [
    "od.download('https://www.kaggle.com/c/cassava-disease/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip zipped file which is already downloaded from a source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with zipfile.ZipFile(\"/Users/paulosgidyelew/Desktop/cassava-classification-capstone/notebooks/cassava-disease/train.zip\", 'r') as zip_file:\n",
    "    zip_file.extractall(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cgm', 'cmd', 'healthy', 'cbb', 'cbsd']"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd = file_path+'train'\n",
    "os.listdir(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbb = os.path.join(file_path+'train/cbb')\n",
    "cbsd = os.path.join(file_path+'train/cbsd')\n",
    "cgm = os.path.join(file_path+'train/cgm')\n",
    "cmd = os.path.join(file_path+'train/cmd')\n",
    "healthy = os.path.join(file_path+'train/healthy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = {key:value for key,value in zip(['cbb',\n",
    "                                            'cbsd',\n",
    "                                            'cgm',\n",
    "                                            'cmd',\n",
    "                                            'healthy'],\n",
    "                                            [os.listdir(cbb),\n",
    "                                            os.listdir(cbsd),\n",
    "                                            os.listdir(cgm),\n",
    "                                            os.listdir(cmd),\n",
    "                                            os.listdir(healthy)])}\n",
    "data1 = pd.DataFrame(train_json['cbb'],columns=['image'])\n",
    "data2 = pd.DataFrame(train_json['cbsd'],columns=['image'])\n",
    "data3 = pd.DataFrame(train_json['cgm'],columns=['image'])\n",
    "data4 = pd.DataFrame(train_json['cmd'],columns=['image'])\n",
    "data5 = pd.DataFrame(train_json['healthy'],columns=['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['label'] = 0\n",
    "data2['label'] = 1\n",
    "data3['label'] = 2\n",
    "data4['label'] = 3\n",
    "data5['label'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>train-cmd-2535.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>train-cmd-560.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>train-cmd-1922.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>train-cbb-371.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>train-cmd-1666.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   image  label\n",
       "634   train-cmd-2535.jpg      3\n",
       "1151   train-cmd-560.jpg      3\n",
       "1281  train-cmd-1922.jpg      3\n",
       "19     train-cbb-371.jpg      0\n",
       "1075  train-cmd-1666.jpg      3"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data1,data2,data3,data4,data5])\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\n",
    "                0:'CBB',\n",
    "                1:'CBSD',\n",
    "                2:'CGM',\n",
    "                3:'CMD',\n",
    "                4:'Healthy'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(labels_dict, open(file_path+'label_dict.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path+'label_dict.json') as file:\n",
    "    str_labels = json.load(file)\n",
    "    str_labels = {int(k):v for k,v in str_labels.items()}\n",
    "data['label_name'] = data.label.map(str_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train-cbb-166.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>CBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train-cbb-172.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>CBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train-cbb-199.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>CBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train-cbb-358.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>CBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train-cbb-364.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>CBB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               image  label label_name\n",
       "0  train-cbb-166.jpg      0        CBB\n",
       "1  train-cbb-172.jpg      0        CBB\n",
       "2  train-cbb-199.jpg      0        CBB\n",
       "3  train-cbb-358.jpg      0        CBB\n",
       "4  train-cbb-364.jpg      0        CBB"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5656, 3)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(file_path+'train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label_name = data.label_name.astype(str).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5656 entries, 0 to 315\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   image       5656 non-null   object  \n",
      " 1   label       5656 non-null   int64   \n",
      " 2   label_name  5656 non-null   category\n",
      "dtypes: category(1), int64(1), object(1)\n",
      "memory usage: 138.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The different classes of the cassava leaf images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CBB', 'CBSD', 'CGM', 'CMD', 'Healthy']\n",
       "Categories (5, object): ['CBB', 'CBSD', 'CGM', 'CMD', 'Healthy']"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The following plot shows data distribution of the classes. As we can see the dataset is imbalanced with CMD being the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data = data, x=data.label_name)\n",
    "plt.xlabel('Leaf Category')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data distribution among the four classes in a data frame   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cbb</th>\n",
       "      <th>cbsd</th>\n",
       "      <th>cgm</th>\n",
       "      <th>cmd</th>\n",
       "      <th>healthy</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Count</th>\n",
       "      <td>466</td>\n",
       "      <td>1443</td>\n",
       "      <td>773</td>\n",
       "      <td>2658</td>\n",
       "      <td>316</td>\n",
       "      <td>5656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cbb  cbsd  cgm   cmd  healthy  total\n",
       "Count  466  1443  773  2658      316   5656"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_len = [i for i in [len(os.listdir(cbb)),\n",
    "                        len(os.listdir(cbsd)),\n",
    "                        len(os.listdir(cgm)),\n",
    "                        len(os.listdir(cmd)),\n",
    "                        len(os.listdir(healthy))]]\n",
    "dict = {key: value for key, value in zip(['cbb','cbsd','cgm','cmd','healthy'],data_len)}\n",
    "data_info =pd.DataFrame(dict,index=['Count'])\n",
    "total = pd.DataFrame({'total':len(data)},index=['Count'])\n",
    "total.columns = ['total']\n",
    "data_info=pd.concat([data_info,total],axis=1)\n",
    "data_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save data frame as csv file in local drive for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info.to_csv(file_path+'data_info.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a data frame of pixel values of the image data for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[data.label_name == 'CMD']\n",
    "pxls,labels = [],[]\n",
    "for idx, (img_id, label) in enumerate(zip(sample.image, sample.label)):\n",
    "    image = cv2.imread(os.path.join(path, \"cmd\", img_id))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    pxls.append(image.shape)\n",
    "    labels.append(img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cmd_img</th>\n",
       "      <th>dpi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train-cmd-1992.jpg</td>\n",
       "      <td>(500, 500, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train-cmd-2243.jpg</td>\n",
       "      <td>(500, 666, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train-cmd-2525.jpg</td>\n",
       "      <td>(500, 500, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train-cmd-1038.jpg</td>\n",
       "      <td>(666, 500, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train-cmd-2531.jpg</td>\n",
       "      <td>(500, 594, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              cmd_img            dpi\n",
       "0  train-cmd-1992.jpg  (500, 500, 3)\n",
       "1  train-cmd-2243.jpg  (500, 666, 3)\n",
       "2  train-cmd-2525.jpg  (500, 500, 3)\n",
       "3  train-cmd-1038.jpg  (666, 500, 3)\n",
       "4  train-cmd-2531.jpg  (500, 594, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pxl = {label: pxl for label, pxl in  zip(labels,pxls)} \n",
    "pxl_frame = pd.DataFrame({'cmd_img':labels,'dpi':pxls}) \n",
    "pxl_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2658 entries, 0 to 2657\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   cmd_img  2658 non-null   object\n",
      " 1   dpi      2658 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 41.7+ KB\n"
     ]
    }
   ],
   "source": [
    "pxl_frame.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel size of each image in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,5))\n",
    "x = [x[0] for x in pxl_frame.dpi]\n",
    "y = [x[1] for x in pxl_frame.dpi]\n",
    "plt.plot(x,y,'r+')\n",
    "plt.xlabel('pix')\n",
    "plt.ylabel('pix')\n",
    "plt.title('CMD Leaf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Images of class CMD for visual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[data.label_name == 'CMD'].sample(9)\n",
    "fig, ax = plt.subplots(int(np.ceil(len(sample)/3)),3,figsize=[15, 20])\n",
    "ax = ax.ravel()\n",
    "for idx, img_id in enumerate(sample.image):\n",
    "    image = cv2.imread(os.path.join(path, \"cmd\", img_id))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image,dsize=(300,300))\n",
    "    ax[idx].set_title(img_id.split('-')[1])\n",
    "    ax[idx].imshow(image)\n",
    "    ax[idx].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color spaces in Image Processing\n",
    "\n",
    "<p style='text-align: justify'>In images processing algorithms one can use various colour spaces based on the better fits to the purpose of the application. These color spaces include RGB, VHS, LAB, HSI and Y'CbCr colour spaces.<br>Colors can provide relevant information in image processing applications like classification of cassava leaves as healthy or unhealthy to identify important edges or other features. For instance, if there is an edge (a step change in pixel value) in hue that is hard to detect in a gray-scale image, or if we need to identify objects of known hue (yellowish or orange color of sick plant leaf and green leaves of healthy cassava plant), color information is useful. If we don't need color, then we can consider it as noise and we can go for the single channel gray-scale image in our algorithm.Thus, in our image classification algorithm, we are going to use RGB color space,however, processing multi channel images comes with its costs:</p>\n",
    "\n",
    "<ol>\n",
    "\n",
    "<li>Computational Speed: \n",
    "\n",
    "<p style='text-align: justify'>With parallel computing capabilities of modern computers, it's possible to perform simple pixel-by-pixel processing of a megapixel image in milliseconds. However, whatever processing time is required to manipulate the image or pool some useful data from it, cost still will be higher compared to one channel (gray-scale) image processing. If we make the rule-of-thumb assumption, processing a three-channel color image, such that in the RGB color space, it takes three times or maybe longer as long as processing a gray-scale image, since we may create a separate luminance channel (luma is a mix of about 60% green, 30% red and 10% blue).</p></li>\n",
    "\n",
    "<li>Difficulty to Visualize:\n",
    "\n",
    "<p style='text-align: justify'>In RGB, HSI, Lab, and other color spaces visualization is much harder since there are additional dimensions that the standard human brain can't comprehend.</p></li>\n",
    "\n",
    "<li>Complexity in modeling:\n",
    "\n",
    "<p style='text-align: justify'>Finding edges and features based on luminance and chrominance will add complexity to the image processing model (luminance represents the brightness in an image, while the chrominance represent the color information). In multi-channel images processing, detail perception is obtained from the luminance component of each pixel almost exclusively, because the human vision system is not well suited to detect structures defined by varying chrominance values.</p></li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[data.label_name == 'CMD'].sample(9)\n",
    "plt.figure(figsize=(10, 8))\n",
    "pixel_values =[]\n",
    "for idx, (img_id, label) in enumerate(zip(sample.image, sample.label_name)):\n",
    "    image = cv2.imread(os.path.join(path, \"cmd\", img_id))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image,dsize=(300,300))\n",
    "    pixel_values.append(image[:,:,0][0])\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(label+' -'+' '+'color: RGB')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(image[:,:,0], cmap='Reds')\n",
    "    plt.title(label+' -'+' '+'color: Red')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.imshow(image[:,:,1], cmap='Greens')\n",
    "    plt.title(label+' -'+' '+'color: Green')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.imshow(image[:,:,2], cmap='Blues')\n",
    "    plt.title(label+' -'+' '+'color: Blue')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><p style='text-align: justify; color: blue'>The following pixel values are only one axis (300 pixels out of 90,000 pixels) of the red channel of the image and the values are between 0 and 255 where 0 is for absolutely <b>black</b> and 255 for absolutely <b>white</b>. In other words, the smaller the closer the number to 0 the darker the shade while the closer the number to 255 the lighter or the whiter the shade. The red channel of the image contains about 300x300 pixels alone for the three channels will be a multiple of 3 of that figure.</p>\n",
    "\n",
    "In summary:\n",
    "\n",
    "+ Images are stored in the form of a matrix or tensor, where these numerical values in these matrix are known as ixel  values.\n",
    "+ These pixel values represent the intensity of each pixel.\n",
    "+ Zero represents black and 255 represents white.\n",
    "+ The matrix is known as the channel and in the case of a gray-scale image, we have only one channel and in RGB we have three channels.\n",
    "+ In single channel we need only one byte or `8 bits unsigned integers` of storage for each pixel and for three channels we need 3bytes for each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([185, 235, 207, 218, 214, 229, 217, 163, 184, 188, 167, 210, 208,\n",
       "       201, 179, 207, 109, 127, 156, 139, 157, 185, 205, 113, 155, 146,\n",
       "       161, 184, 179, 194, 242, 196, 196, 191, 191, 137,  95, 182, 128,\n",
       "       100,  68,  65, 128, 100,  92,  80,  73,  65,  60,  53,  49,  45,\n",
       "        42,  44,  54,  49,  52,  49,  44,  43,  45,  45,  44,  41,  50,\n",
       "        59,  64,  65,  58,  55,  61,  82, 119, 144, 103,  75,  92, 159,\n",
       "       185, 188, 184, 170, 133,  71,  39,  34,  28,  27,  25,  23,  27,\n",
       "        30,  31,  33,  34,  38,  47,  55,  55,  52,  48,  47,  47,  49,\n",
       "        52,  50,  61,  64,  63,  52,  36,  27,  23,  20,  19,  20,  22,\n",
       "        25,  26,  25,  25,  25,  31,  36,  51,  53,  40,  43,  46,  48,\n",
       "        52,  53,  59,  54,  34,  31,  31,  39,  39,  37,  32,  48,  53,\n",
       "        43,  40,  41,  37,  32,  33,  35,  37,  35,  35,  34,  39,  41,\n",
       "        45,  43,  37,  37,  33,  31,  28,  30,  31,  32,  33,  33,  38,\n",
       "        37,  31,  28,  23,  21,  21,  25,  27,  27,  28,  29,  26,  25,\n",
       "        23,  22,  23,  23,  24,  29,  29,  24,  24,  24,  26,  24,  23,\n",
       "        23,  24,  23,  25,  30,  37,  44,  46,  47,  54,  48,  54,  91,\n",
       "       119, 112, 135, 162, 163, 150, 147, 171, 182, 181, 176, 178, 181,\n",
       "       176, 169, 174, 190, 202, 207, 206, 197, 186, 177, 183, 183, 176,\n",
       "       165, 154, 143, 109,  81,  79,  57,  49,  48,  66,  89,  75,  67,\n",
       "        79, 112, 160, 155,  71,  57,  57,  54,  45,  63, 153, 205, 207,\n",
       "       153,  97,  97,  94,  99, 117, 123, 118, 121, 126, 138, 151, 156,\n",
       "       143, 110,  94,  89,  88,  83,  82,  89,  91, 108, 122, 132, 123,\n",
       "       107, 107, 110, 123, 124,  98,  76,  54,  40,  49,  91,  51,  41,\n",
       "        40], dtype=uint8)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Maps\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>color map</th>\n",
    "        <th>description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>hsv</td>\n",
    "        <td>cyclic red-yellow-green-cyan-blue-magenta-red, formed by changing the hue component in the HSV color space</td>  \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>bwr</td>\t\n",
    "        <td>diverging blue-white-red</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gray\t</td>\t\n",
    "        <td>sequential linearly-increasing black-to-white gray-scale</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Source: [Kite](https://www.kite.com/python/docs/matplotlib.pyplot.colormaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[data.label_name == 'CMD'].sample(9)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for idx, (img_id, label) in enumerate(zip(sample.image, sample.label_name)):\n",
    "  \n",
    "    image = cv2.imread(os.path.join(path, \"cmd\", img_id))\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_BGR2LAB)\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(label+'-'+' '+'color: LAB')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(image[:,:,0], cmap='gray')\n",
    "    plt.title(label+'-'+' '+'color: gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.imshow(image[:,:,1], cmap='hsv')\n",
    "    plt.title(label+'-'+' '+'color: hsv')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.imshow(image[:,:,2], cmap='bwr')\n",
    "    plt.title(label+'-'+' '+'color: bwr')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacian Transformation:\n",
    "\n",
    "Our dataset contains partly poor quality images. We need to find a solution to filter out these images from our dataset, since they will negatively impact the accuracy of our model. The laplace transform function is used to resolve this problem. In the following we will try to explain how this function would be applied and how it works.\n",
    "\n",
    "Laplace filtering method used to identify and highlight fine edges based on the 2nd derivative.\n",
    "\n",
    "`OpenCV.Laplacian(gray_img, ksize, scale)` returns filtered image\n",
    "\n",
    "Parameters:\n",
    "\n",
    "gray_img - Grayscale image data\n",
    "ksize - apertures size used to calculate 2nd derivative filter, specifies the size of the kernel (must be an odd integer: 1,3,5...)\n",
    "scale - scaling factor applied (multiplied) to computed Laplacian values (scale = 1 is unscaled)\n",
    "\n",
    "Context:\n",
    "\n",
    "It is used to define edges around objects. The function calculates the Laplacian of the source image by adding up the second x and y derivatives calculated using the Sobel operator:\n",
    "\n",
    "$Laplace(f) = \\dfrac{\\partial^{2} f}{\\partial x^{2}} + \\dfrac{\\partial^{2} f}{\\partial y^{2}}$\n",
    "\n",
    "When ksize = 1, the Laplacian is computed by filtering the image with the following 3×3 aperture:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "0 & 1 & 0\\\\\n",
    "1 & -4 & 1\\\\\n",
    "0 & 1 & 0 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "<p style=\"text-align: justify\">Function described in this section is used to perform linear or non-linear filtering operations on 2D images. It means that for each pixel location (x, y) in the source image (normally, rectangular), its neighborhood is considered and used to compute the response. In case of a linear filter, it is a weighted sum of pixel values. In case of morphological operations, it is the minimum or maximum values, and so on. The computed response is stored in the destination image at the same location (x, y). It means that the output image will be of the same size as the input image. Normally, the functions support multi-channel arrays, in which case every channel is processed independently. Therefore, the output image will also have the same number of channels as the input one.</p>\n",
    "\n",
    "Depth combinations:\n",
    "\n",
    "|Input depth (src.depth())\t|Output depth (ddepth)|\n",
    "|---|---|\n",
    "|CV_8U\t|-1/CV_16S/CV_32F/CV_64F|\n",
    "|CV_16U/CV_16S\t|-1/CV_32F/CV_64F|\n",
    "|CV_32F\t|-1/CV_32F/CV_64F|\n",
    "|CV_64F\t|-1/CV_64F|\n",
    "\n",
    "<p style=\"text-align: justify\">OpenCV BGR or Gray_scale images have pixel values from 0 to 255 when in CV_8U - corresponds to np.uint8. When we use the Laplacian transform with ddepth (desired depth of the destination image), we set it to OpenCV.CV_32F or OpenCV.CV_64F.</p> \n",
    "\n",
    "For more details [click here](https://docs.opencv.org/4.1.0/d4/d86/group__imgproc__filter.html#filter_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def laplacian(threshold, path):\n",
    "    bad_img,lap_val = [],[]\n",
    "    for img in glob.glob(path):\n",
    "        image = cv2.imread(img)\n",
    "        img_gry = cv2.cvtColor(image, cv2.IMREAD_GRAYSCALE)\n",
    "        lap_var = cv2.Laplacian(img_gry,cv2.CV_32F,ksize=1).var()\n",
    "        if lap_var < threshold:\n",
    "            bad_img.append(img)\n",
    "            lap_val.append(lap_var)\n",
    "    bad_img_dict = {img:val for img,val in zip(bad_img,lap_val)}\n",
    "    return bad_img_dict\n",
    "\n",
    "bad_img_dict = laplacian(75, \"../data/cassava-disease/train/cbb/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>laplacian_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train-cbb-365.jpg</td>\n",
       "      <td>27.239807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train-cbb-167.jpg</td>\n",
       "      <td>65.157829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train-cbb-399.jpg</td>\n",
       "      <td>54.976665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train-cbb-311.jpg</td>\n",
       "      <td>33.626156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train-cbb-263.jpg</td>\n",
       "      <td>51.409893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_id  laplacian_var\n",
       "0  train-cbb-365.jpg      27.239807\n",
       "1  train-cbb-167.jpg      65.157829\n",
       "2  train-cbb-399.jpg      54.976665\n",
       "3  train-cbb-311.jpg      33.626156\n",
       "4  train-cbb-263.jpg      51.409893"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_img_lap = [i.split('/')[5] for i in bad_img_dict.keys()]\n",
    "lap_val = bad_img_dict.values()\n",
    "bad_img_S = pd.Series(bad_img_lap)\n",
    "lap_var_S = pd.Series(lap_val)\n",
    "\n",
    "bad_img_frm = pd.concat([bad_img_S,lap_var_S],axis=1)\n",
    "bad_img_frm.columns = ['image_id','laplacian_var']\n",
    "bad_img_frm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot blurry images of class \"cbb\"\n",
    "\n",
    "def laplacian_plot(rows=4,cols=4):\n",
    "    fig, ax = plt.subplots(int(np.ceil(len(bad_img_dict.keys())/cols)),rows,figsize=[15, 20])\n",
    "    ax = ax.ravel()\n",
    "    for idx, img in enumerate(bad_img_dict.keys()):\n",
    "        image = cv2.imread(img)\n",
    "        image = cv2.resize(image,dsize=(250,250))\n",
    "        ax[idx].set_title(img.split('/')[5])\n",
    "        ax[idx].axis('off')\n",
    "        ax[idx].imshow(image)\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "laplacian_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Visualization of RGB Image\n",
    "\n",
    " In RGB, HSI, Lab, and other color spaces, visualization is difficult since there are additional dimensions that the standard human brain can't comprehend. The following 3D map shows how the tensor of a 3 channel image look like. The x, y and z axes represent pixel values in their respective axes between 0 and 255 as 8 bit unsigned integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(os.path.join(path,bad_img_frm.image_id[0].split('-')[1]+'/'+bad_img_frm.image_id[0]))\n",
    "image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image,dsize=(300,300))\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "\n",
    "# First subplot\n",
    "ax = fig.add_subplot(2, 1, 1, projection='3d')\n",
    "ax.set_title('3D color space of an image in RGB scale',fontsize=16)\n",
    "y = range( image.shape[0] )\n",
    "x = range( image.shape[1] ) \n",
    "X, Y = np.meshgrid(x, y)\n",
    "ax.plot_surface(X, Y, image[:,:,2])\n",
    "\n",
    "# Second subplot\n",
    "ax = fig.add_subplot(2, 3, 3)\n",
    "ax.set_title('Image in  RGB color space', fontsize=16)\n",
    "ax.imshow(image)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "420d7ed6979cd352c66806428606ddfc9cefeeb84853fbc86647921a84d6eb24"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
